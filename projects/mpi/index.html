
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>Stereo Magnification: Learning View Synthesis using Multiplane Images</title>
		<meta property="og:image" content="http://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/images/teaser.png"/>
		<meta property="og:title" content="Stereo Magnification: Learning View Synthesis using Multiplane Images" />
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:36px">Stereo Magnification: Learning View Synthesis using Multiplane Images</span>
	</center>
    
	<br>
  	<table align=center width=700px>
  	 <tr>
		<td align=center width=100px>
		<center>
		<span style="font-size:18px"><a href="http://www.cs.berkeley.edu/~tinghuiz/">Tinghui Zhou</a><sup>1</sup></span>
		</center>
		</td>

		<td align=center width=100px>
		<center>
		<span style="font-size:18px">Richard Tucker<sup>2</sup></span>
		</center>
		</td>

		<td align=center width=100px>
		<center>
		<span style="font-size:18px">John Flynn<sup>2</sup></span>
		</center>
		</td>

		<td align=center width=100px>
		<center>
		<span style="font-size:18px">Graham Fyffe<sup>2</sup></span>
		</center>
		</td>

		<td align=center width=100px>
		<center>
		<span style="font-size:18px"><a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a><sup>2</sup></span>
		</center>
		</td>

	 </tr>
	</table>

	<table align=center width=700px>
  	 <tr>
		<td align=center width=100px>
		<center>
		<span style="font-size:20px">UC Berkeley<sup>1</sup>, Google<sup>2</sup></span>
		</center>
		</td>
	 </tr>
	</table>

	<table align=center width=700px>
  	 <tr>
		<td align=center width=100px>
		<center>
		<span style="font-size:20px">In SIGGRAPH, 2018</span>
		</center>
		</td>
	 </tr>
	</table>

	<!--

	<table align=center width=400px>
		<tr>
	  	              <td align=center width=150px>
	  					<center>
	  						<span style="font-size:20px"><a href='https://github.com/shubhtuls/drc'>[GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=150px>
	  					<center>
	  						<span style="font-size:20px"><a href='#'>[Paper]</a></span>
		  		  		</center>
		  		  	  </td>
  		</tr>
	</table>

	-->          
  		  <br>
  		  <table align=center width=900px>
  			  <tr>
  	              <td width=600px>
  					<center>
  	                	<a href="./images/teaser.png"><img src = "./images/teaser.png" height="300px"></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	  <!--             <td width=600px>
  					<center>
  	                	<span style="font-size:14px"><i> <span style="font-weight:bold">Top</span>: We measure the consistency between a (predicted) 3D shape and an observation image using a differentiable ray consistency formulation. <span style="font-weight:bold">Right</span>: Sample results obtained by applying our framework on ShapeNet dataset using multiple color images as supervision for training. We show the input RGB image and the visualize the 3D shape predicted using our multi-view supervised CNN from two novel views.</i>
					</center>
  	              </td> -->
  	          </tr>
  		  </table>

      	  <br>
          In this paper, we explore an intriguing scenario for view synthesis: extrapolating views from imagery captured by narrow-baseline stereo cameras, including VR cameras and now-widespread dual-lens camera phones. We call this problem <i>stereo magnification</i>, and propose a learning framework that leverages a new layered representation that we call <i>multiplane images</i> (MPIs). Our method also uses a massive new data source for learning view extrapolation: online videos on YouTube. Using data mined from such videos, we train a deep network that predicts an MPI from an input stereo image pair. This inferred MPI can then be used to synthesize a range of novel views of the scene, including views that extrapolate significantly beyond the input baseline.
  		  <br><br>
		  <hr>
		 <!-- <table align=center width=550px> -->
  		  <table align=center width=1100>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
				  <td><img style="height:180px" src="images/paper_thumbnails.png"/></td>
				  <td><span style="font-size:14pt">Stereo Magnification: Learning View Synthesis using Multiplane Images<br><br>
                          <i>Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely</i><br><br>
				  In SIGGRAPH 2018<br>
				  <!-- [hosted on <a href="#">arXiv</a>]</a> -->
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=400px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="http://people.eecs.berkeley.edu/~tinghuiz/papers/siggraph18_mpi.pdf">[PDF (35MB)]</a>
  	              </center></td>

  	              <td><span style="font-size:14pt"><center>
				  	<a href="http://people.eecs.berkeley.edu/~tinghuiz/papers/siggraph18_mpi_lowres.pdf">[Compressed PDF (1MB)] </a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>
		  	<br>

  		  	<hr>

		<center><h1>Code</h1></center>
  		  

  		  <table align=center width=800px>
			  <tr><center> <br>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
				<span style="font-size:28px">&nbsp;<a href='https://github.com/google/stereo-magnification'>[GitHub]</a>

				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table>
      	  <br>
		  <hr>

		  <center><h1>RealEstate10K Dataset</h1></center>
  		  

  		  <table align=center width=800px>
			  <tr><center> <br>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
				<span style="font-size:28px">&nbsp;<a href='https://google.github.io/realestate10k/'>[Link]</a>
					<br>
					<br>
				<td><span style="font-size:14pt">Note: due to data restrictions, we are unable to release the same version used in the paper. However, we are working on updating the results using this public version.

				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table>
      	  <br>
		  <hr>

		  <center><h1>Sample Results from the Paper</h1></center>
  		  

  		  <table align=center width=800px>
			  <tr><center> <br>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
				<span style="font-size:28px">&nbsp;<a href='https://drive.google.com/open?id=1CZGJxRl0GK0js0MbL7cn7tHtdRrtnjOB'>[Google drive Link] (772 MB)</a>

				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table>
      	  <br>
		  <hr>

  		  <a name="results"></a>
			<center><h1>Supplemental video</h1></center>
			<!-- Our Differentiable Ray Consistency (DRC) formulation allows us to learn single-view 3D reconstruction using varying kinds of multi-view observations and can be applied to diverse settings.-->

			<table align=center width=500px>
				<tr height="300px">
					<td valign="top" width=400px>
					<center>
					<iframe width="640" height="360" src="https://www.youtube.com/embed/oAKDhHPwSUE" frameborder="0" allowfullscreen></iframe>
					</center>
					</td>

				</tr>
			</table>
		<br>
	  
	  	<hr>
  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Authors' Note</h1></center>
	  		  After the publication of this work, it came to the authors' attention that another earlier representation similar to the MPI was explored in <i>Stereo Matching with Transparency and Matting, Richard Szeliski and Polina Golland, IJCV 1999</i>. We encourage citing Szeliski & Golland as well if you find MPIs useful in your research.
<br>
<hr>
	  		  <center><h1>Acknowledgements</h1></center>
	  		  We thank the anonymous reviewers for their valuable comments and Shubham Tulsiani for helpful discussions. This work was done while TZ was an intern at Google. This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.

			</left>
		</td>
		</tr>
		</table>
		<br><br>
</body>
</html>
